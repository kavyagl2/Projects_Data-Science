# -*- coding: utf-8 -*-
"""Copy of Fraud_transactions_detection

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g2hU8UnQiMRTd4cmcSyjI5oU0tWa8TiA
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'fraudulent-transactions-data:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1964310%2F3240868%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240224%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240224T110145Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D16846152bf0a583182d867c89c83a4da8bb2e9b6438d7d46b4cc494be3a3fad402028a3589589f8b17bd14bd2f3a60756253ec412699200f78ad0a216735088c9d37891236be16021af4a4fcc3b5a3d0ed9e27d146a51b45f4d6e650ef257767287a2903335fd2cbb8997f377a245051b4b28205c54fab5e483ef6babee1538a36282bbb6615d65aab61168c7eb787cbd0d0cd4ed9f5bec5e32562d7874ba318ec3babe75d372df308b74701b78eb73b1128c67b180f0b561caa172dc82c6f51a57192ecfa6dec5dae29f4203e2f9a32201afa5b228440db6d37493c213d8b414f7c34c0c16a5fd57a7aeb705e5673584bb05b93aef677c2017dcebd1a5e522b'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

import numpy as np
import pandas as pd
import torch

if torch.cuda.is_available():
    device='cuda'
else:
    device='cpu'

#reading the data
df= pd.read_csv("/kaggle/input/fraudulent-transactions-data/Fraud.csv")
#getting the number of rows and columns
df.shape

#head of the data
df.head(200)

#tail of the data
df.tail(200)

#checking for null values if any
df.isnull().values.any()

#collect more information about the data
df.info()

#legit transactions
legit = len(df[df.isFraud==0])
#fraud transactions
fraud = len(df[df.isFraud==1])

#legit transactions percentage
legit_perc = round((legit/(legit+fraud))*100,2)

#fraud transactions percentage
fraud_perc = round((fraud/(legit+fraud))*100,2)

print("Number of legit transactions: ",legit)
print("Number of fraud transactions: ",fraud)
print(f"{legit_perc}% are legit transactions in this dataset")
print(f"{fraud_perc}% are fraud transactions in this dataset")

"""Since we are dealing with the higly imbalanced data where legit transactions cover around 99.87% while fraud transactions are just 0.13% of the data, the best way to reach the final decision is to work with decision trees and random forests."""

#Merchants
Y = df[df["nameDest"].str.contains('M')]
Y.head()

import seaborn as sns
import matplotlib.pyplot as plt

"""since *type* , *nameOrig* and *nameDest* are objective in nature
we will drop them and create a copy of original data
for vizualizing the correlation heatmap"""

df_corr = df.drop(columns = ["type","nameOrig","nameDest"])
corr = df_corr.corr()
plt.figure(figsize=(10,5))
sns.heatmap(corr,annot=True)

plt.figure(figsize = (5,8))
labels = ["Legit","Fraud"]
count_classes = df.value_counts(df["isFraud"],sort=True)
count_classes.plot(kind = "bar", rot = 0)
plt.title("Bar plot for trasanctions")
plt.ylabel("count")
plt.xticks(range(2), labels)
plt.show()

#creating a copy for training and testing the data
new_df = df.copy()
new_df.head()

#checking how many are of object type
list_obj = new_df.select_dtypes(include='object').columns
print(list_obj)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

for feat in list_obj:
    new_df[feat] = le.fit_transform(new_df[feat].astype(str))

print(new_df.info())

new_df.head()

#checking for multicollinearity in data by using VIF (variance inflation factor)
from statsmodels.stats.outliers_influence import variance_inflation_factor

def cal_vif(new_df):
    vif = pd.DataFrame()
    vif["variables"] = new_df.columns
    vif["VIF"] = [variance_inflation_factor(new_df.values,i) for i in range(new_df.shape[1])]
    return(vif)

cal_vif(new_df)

"""NewbalanceOrig and oldbalanceOrg are highly correlated, similar goes with
oldbalancedest and newbalanedest.
Also looking over the information we can see that namedest and nameOrig are connected to each other"""

#combining features with high collinearity and dropping individual ones

new_df["orig_amount"] = new_df.apply(lambda x: x["oldbalanceOrg"]- x["newbalanceOrig"], axis = 1)
new_df["dest_amount"] = new_df.apply(lambda x: x["oldbalanceDest"]- x["newbalanceDest"], axis = 1)
new_df["TransactionPath"] = new_df.apply(lambda x: x["nameDest"]+ x["nameOrig"], axis = 1)

#dropping individual columns
new_df = new_df.drop(['oldbalanceOrg','newbalanceOrig','oldbalanceDest','newbalanceDest','step','nameOrig','nameDest'],axis=1)

cal_vif(new_df)

#generating correlation matrix of the new updated dataframe
corr=new_df.corr()

plt.figure(figsize=(10,6))
sns.heatmap(corr,annot=True)

#importing libraries and modules for building our model
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
import itertools
from collections import Counter
import sklearn.metrics as metrics
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

#scaling and normalizing our data for better model accuracy and standardization
scaler = StandardScaler()
new_df["NormalizedAmount"] = scaler.fit_transform(new_df["amount"].values.reshape(-1, 1))
new_df.drop(["amount"], inplace= True, axis= 1)

Y = new_df["isFraud"]
X = new_df.drop(["isFraud"], axis= 1)

#spliting the data in the training and testing dataset
(X_train, X_test, Y_train, Y_test) = train_test_split(X, Y, test_size= 0.3, random_state= 42)

print("Shape of X_train: ", X_train.shape)
print("Shape of X_test: ", X_test.shape)

#DECISION TREE
decision_tree = DecisionTreeClassifier()
decision_tree.fit(X_train, Y_train)

Y_pred_dt = decision_tree.predict(X_test)
decision_tree_score = decision_tree.score(X_test, Y_test)*100

#RANDOM FORESTS
random_forest = RandomForestClassifier(n_estimators = 100)
random_forest.fit(X_train, Y_train)

Y_pred_rf = random_forest.predict(X_test)
random_forest_score = random_forest.score(X_test, Y_test)*100

#score of classifiers
print("Decision Tree Score: ", decision_tree_score)
print("Random Forests Score: ", random_forest_score)

#confusion matrix for decision tree
print("TP, FP, TN, FN - DECISION TREE")
tn, fp, fn, tp = confusion_matrix(Y_test, Y_pred_dt).ravel()

print(f'True Positives: {tp}')
print(f'False Positives: {fp}')
print(f'True Negatives: {tn}')
print(f'False Negatives: {fn}')

confusion_matrix_dt = confusion_matrix(Y_test, Y_pred_dt.round())
print("\nConfusion Matrix - Decision Tree")
print(confusion_matrix_dt)

print("---------------------------------------------------------------------------------------------------------")
#confusion matrix for random forests
print("TP, FP, TN, FN - RANDOM FORESTS")
tn, fp, fn, tp = confusion_matrix(Y_test, Y_pred_rf).ravel()

print(f'True Positives: {tp}')
print(f'False Positives: {fp}')
print(f'True Negatives: {tn}')
print(f'False Negatives: {fn}')

confusion_matrix_rf = confusion_matrix(Y_test, Y_pred_rf.round())
print("\nConfusion Matrix - Random Forest")
print(confusion_matrix_rf)
print("---------------------------------------------------------------------------------------------------------")

#classification report - dt
classification_dt = classification_report(Y_test, Y_pred_dt)
print("Classification Report - Decision Tree")
print(classification_dt)


print("---------------------------------------------------------------------------------------------------------")

#classification report - rf
classification_rf = classification_report(Y_test, Y_pred_rf)
print("Classification Report - Random Forests")
print(classification_dt)

#confusion matrix - decision tree vizualization
decision_cf = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix_dt)
decision_cf.plot()
plt.title("Decision Tree Confusion Matrix")
plt.show()


#confusion matrix - random forests vizualization
random_forest_cf = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix_rf)
random_forest_cf.plot()
plt.title("Random Forest Confusion Matrix")
plt.show()

# AUC ROC - DT
# calculate the fpr and tpr for all thresholds of the classification

fpr, tpr, threshold = metrics.roc_curve(Y_test, Y_pred_dt)
roc_auc = metrics.auc(fpr, tpr)

plt.title('ROC - DT')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()


# AUC ROC - RF
# calculate the fpr and tpr for all thresholds of the classification

fpr, tpr, threshold = metrics.roc_curve(Y_test, Y_pred_rf)
roc_auc = metrics.auc(fpr, tpr)

plt.title('ROC - RF')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

