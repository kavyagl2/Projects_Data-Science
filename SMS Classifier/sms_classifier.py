# -*- coding: utf-8 -*-
"""SMS Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SYZPZ01bhQ40yUJCqM20Yf-rUoF2BevL

**SMS CLASSIFIER**


---

**Main technologies used:** Python, Sklearn

**1. Steps for Data Processing:**


* Importing the required packages
* Loading the dataset
* Removing the unwanted data columns
* Preprocessing and exploring the dataset
* Building the wordcloud to see which message is spam and which is ham
* Removing the stopwords and punctuations
* Converting the message data into vectors

**2. Steps for building a sms spam classification model**


* Spliting the data into test and train data sets
* Using the Sklearn built in classifiers to build the model
* Training the data on the model
* Make predictions on  new data



---

**IMPORTING THE REQUIRED PACKAGES**
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import sklearn
import nltk
import seaborn as sns
from wordcloud import WordCloud
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold, cross_val_score, learning_curve
from sklearn.metrics import confusion_matrix

"""**LOADING THE DATASET**"""

words_data = pd.read_csv('/content/spam (1).csv', encoding = 'latin-1')
words_data.head()

"""**REMOVING THE UNWANTED DATA COLUMNS**"""

words_data = words_data.drop(["Unnamed: 2","Unnamed: 3","Unnamed: 4"], axis = 1)

words_data['Category'].value_counts()

"""**PREPROCESSING AND EXPLORING THE DATASET**"""

#download Punkt Tokenizer model from Nltk
nltk.download('punkt')

import warnings
warnings.filterwarnings('ignore')

"""**BUILDING WORDCLOUDS TO CLASSIFY WHICH WORD IS SPAM and WHICH IS HAM**"""

ham_words = ''
spam_words = ''

#creating a corpus for spam words
for val in words_data[words_data['Category'] == 'spam'].Message:
  text = val.lower()
  tokens = nltk.word_tokenize(text)
  for words in tokens:
    spam_words+= words+ ' '

#creating a corpus for ham words
for val in words_data[words_data['Category'] == 'ham'].Message:
  text = val.lower()
  tokens = nltk.word_tokenize(text)
  for words in tokens:
    ham_words+= words+ ' '

#creating a spam word and ham word cloud

#SPAM WORD CLOUD
spam_wordcloud = WordCloud(width = 500, height = 500).generate(spam_words)

plt.figure(figsize = (10,8), facecolor= 'r')
plt.title("WordCloud for Spam words")
plt.imshow(spam_wordcloud)
plt.axis('off')
plt.tight_layout(pad = 0)
plt.show()

#HAM WORD CLOUD
ham_wordcloud = WordCloud(width = 500, height = 500).generate(ham_words)

plt.subplots_adjust()
plt.figure(figsize = (10,8), facecolor= 'g')
plt.title("WordCloud for Ham words")
plt.imshow(ham_wordcloud)
plt.axis('off')
plt.tight_layout(pad = 0)
plt.show()

#for easy interpreation by machine let's convert ham and spam to 0 and 1 respectively
words_data = words_data.replace(['ham','spam'],[0,1])
words_data.head()

"""**REMOVING PUNCTUATIONS AND STOPWORDS**"""

nltk.download('stopwords')

#remove the punctuations and stopwords
import string
def text_process(Message):
  Message = Message.translate(str.maketrans('','',string.punctuation))
  Message = [word for word in Message.split() if word.lower() not in stopwords.words('english')]

  return " ".join(Message)

words_data["Message"] = words_data["Message"].apply(text_process)
words_data.head()

#create different dataframes for both label and messsage column from dataset
Message = pd.DataFrame(words_data['Message'])
Category = pd.DataFrame(words_data['Category'])

"""**CONVERTING WORDS INTO VECTOR**"""

vectorizer = TfidfVectorizer()
vectors = vectorizer.fit_transform(words_data['Message'])
vectors.shape

features = vectors

"""**SPLITTING INTO TEST AND TRAIN SET**"""

#splitting the data set into training and test set

x_train,x_test,y_train,y_test = train_test_split(features, words_data['Category'], test_size =0.15, random_state= 111 )

"""**USING SKLEARN BUILT-IN CLASSIFIERS FOR CLASSIFICATION**


* Logistic Regression
* Support Vector Machine
* Naive Bayes
* Decision Tree
* K-Nearest Neighbour
* Random Forest Classifier

"""

#importing Sklearn packages for building classifiers
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

#initialize multiple classification models
svc = SVC(kernel = 'sigmoid', gamma = 1.0)
knc = KNeighborsClassifier(n_neighbors = 49)
mnb = MultinomialNB(alpha = 0.2)
dtc = DecisionTreeClassifier(min_samples_split=7, random_state = 111)
lrc = LogisticRegression(solver = 'liblinear', penalty = 'l1')
rfc = RandomForestClassifier(n_estimators = 31, random_state = 111)

#create a dictionary of variables and models
clfs = {'SVC' : svc,'KN' : knc, 'NB': mnb, 'DT': dtc, 'LR': lrc, 'RF': rfc}

#fit the data onto the models
def train(clf, features, targets):
  clf.fit(features,targets)

def predict(clf, features):
  return (clf.predict(features))

pred_scores_word_vectors = []

for k,v in clfs.items():
  train(v,x_train,y_train)
  pred = predict(v, x_test)
  pred_scores_word_vectors.append((k, [accuracy_score(y_test,pred)]))

"""**PREDICTIONS USING TFIDF VECTORIZER**"""

pred_scores_word_vectors

"""**MODEL PREDICTIONS**"""

def find_message_spam_ham(x):
  if x==1:
    print("Message is SPAM")

  else:
    print("Message is NOT SPAM")

user_input = [input("Enter the message you want to check: ")]
integers = vectorizer.transform(user_input)

x = mnb.predict(integers)
find_message_spam_ham(x)

"""**CHECKING CLASSIFICATION RESULTS WITH CONFUSION MATRIX**"""

#Using Naive Bayes
y_pred_nb = mnb.predict(x_test)
y_true_nb = y_test
cm = confusion_matrix(y_true_nb, y_pred_nb)
f, ax = plt.subplots(figsize = (5,5))
sns.heatmap(cm, annot = True, linewidths=0.5, linecolor="green", fmt = ".0f", ax=ax)
plt.xlabel("y_pred_nb")
plt.ylabel("y_true_nb")
plt.show()

