# -*- coding: utf-8 -*-
"""Bank_review_Text_Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZPgHLhW1Gy4mfrOTAjI0nCEVi57dV5HO

# Text Analysis of Customer Reviews of Bank

**Objective:** To understand the relation between rating and customer review.  

**Dataset:** The dataset is accessed from HuggingFace Datasets for text classification.

The dataset contains
* Author Name,
* Date,
* Location,
* Name of Bank,
* Rating,
* Text Review,
* Likes

Link to the dataset access: https://huggingface.co/datasets/TrainingDataPro/customers-reviews-on-banks

**Content of Notebook:**

* Exploratory Data Analysis
* Frequent word for negative Review
* Topic Modeling using LDA
* Analysing influential Author's Review
* Using TF-Idf method, NER Model, Bertopic Topic Modeling
"""

# Importing Libraries

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#To access the dataset from HuggingFace dataset we install datasets

! pip install datasets

from datasets import load_dataset

import numpy as np

data= load_dataset('TrainingDataPro/customers-reviews-on-banks')

data

data_tr= load_dataset('TrainingDataPro/customers-reviews-on-banks', split='train')

# Sample observation
data_tr[3908]

"""## Exploratory Data Analysis"""

data_tr.shape

"""**Frequency of Observation based on Banks**"""

dataset_df = pd.DataFrame(data_tr)

# Assuming 'name_of_bank' is the column name containing the names of the banks
bank_frequency = dataset_df['bank'].value_counts()

# Plotting
plt.figure(figsize=(40, 10))
bank_frequency.plot(kind='bar')
plt.title('Frequency of Reviews for Each Bank')
plt.xlabel('Bank Name')
plt.ylabel('Frequency')
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.tight_layout()  # Adjust layout to prevent label cutoff
plt.show()

"""**Rating Distribution for All Observation**"""

ratings_distribution = dataset_df['star'].value_counts().sort_index()

# Plotting
plt.figure(figsize=(4, 4))
ratings_distribution.plot(kind='bar', color='skyblue')
plt.title('Distribution of Ratings for All Observations')
plt.xlabel('Star Rating')
plt.ylabel('Frequency')
plt.xticks(rotation=0)  # Rotate x-axis labels if necessary
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""**Rating Distribution specific to Bank**"""

bank_rating_data = dataset_df.groupby(['bank', 'star']).size().reset_index(name='count')

plt.figure(figsize=(14, 10))

# Use seaborn's FacetGrid to create a faceted plot
g = sns.FacetGrid(bank_rating_data, col="bank", col_wrap=5, height=4, sharey=False)
g.map(sns.barplot, "star", "count", order=[1, 2, 3, 4, 5])
g.set_titles("{col_name}")
g.set_axis_labels("Star Rating", "Frequency")

for ax in g.axes.flat:
    ax.set_xticklabels(['1', '2', '3', '4', '5'])

plt.tight_layout()  # Adjust layout to prevent label cutoff
plt.show()

"""**Based on Location**"""

location_distribution = dataset_df['location'].value_counts()

# Plotting
plt.figure(figsize=(10, 6))
location_distribution.plot(kind='bar', color='skyblue')
plt.title('Distribution of Locations for All Observations')
plt.xlabel('Location')
plt.ylabel('Frequency')
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.tight_layout()
plt.show()

"""**Distribution of Likes**"""

plt.figure(figsize=(10, 6))
plt.hist(dataset_df['like'], bins=100, color='skyblue')
plt.title('Distribution of Likes')
plt.xlabel('Number of Likes')
plt.ylabel('Frequency')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""### Insights from EDA


*   We can observe that rating from all observation is dominated mostly by **negative reviews** across all the banks.
*   We can focus on the negative reviews to identify which **product or service** are customers mostly complainting about.
* The major distribution of number of likes ranges from 0 to 50. Analysiing the text reviews for most number of likes will provide the keywords used by the most influential author.

### Frequent Word for Negative Review
"""

import nltk
nltk.download('punkt')

import nltk
nltk.download('wordnet')

import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize

import re

reviews_rating_1_2 = dataset_df.loc[(dataset_df['star'] == 1) | (dataset_df['star'] == 2), 'text'].dropna()

reviews_rating_1_2_joined = ' '.join(reviews_rating_1_2.tolist())

tokens = nltk.word_tokenize(reviews_rating_1_2_joined)

filtered_tokens = [word for word in tokens if word.lower() not in set(stopwords.words('english'))]

reviews_rating_1_2_filtered = ' '.join(filtered_tokens)

special_chars_pattern = re.compile(r'[^a-zA-Z\s]')

# Remove special characters from the filtered text
reviews_rating_1_2_filtered_cleaned = special_chars_pattern.sub('', reviews_rating_1_2_filtered)

reviews_rating_1_2_filtered_cleaned

corpus = []
for i in range(0, len(reviews_rating_1_2_filtered)):
    review = re.sub('[^a-zA-Z]', ' ', reviews_rating_1_2_filtered[i])
    review = review.lower()
    review = review.split()
    review = ' '.join(review)
    corpus.append(review)

from collections import Counter
import nltk
from nltk import ngrams
from tabulate import tabulate


def frequent_words(reviews, n=20, ngram=1):
    tokens = nltk.word_tokenize(reviews.lower())
    if ngram == 1:
        grams = tokens
    elif ngram == 2:
        grams = list(ngrams(tokens, 2))
    elif ngram == 3:
        grams = list(ngrams(tokens, 3))
    elif ngram == 4:
        grams = list(ngrams(tokens, 4))

    freq_dist = Counter(grams)
    top_n = freq_dist.most_common(n)
    return top_n

# Filter dataset for ratings 1 and 2
#reviews_rating_1_2 = ' '.join(review['text'] for review in dataset_df if review['star'] in [1, 2])

# Get the most frequent words, bi-grams, and tri-grams for ratings 1 and 2
top_words = frequent_words(reviews_rating_1_2_filtered_cleaned, n=20, ngram=1)
top_bi_grams = frequent_words(reviews_rating_1_2_filtered_cleaned, n=20, ngram=2)
top_tri_grams = frequent_words(reviews_rating_1_2_filtered_cleaned, n=20, ngram=3)
top_quad_grams= frequent_words(reviews_rating_1_2_filtered_cleaned, n=20, ngram=4)

# Convert the results into a presentable table format
top_words_table = tabulate(top_words, headers=['Word', 'Frequency'], tablefmt='grid')
top_bi_grams_table = tabulate(top_bi_grams, headers=['Bi-gram', 'Frequency'], tablefmt='grid')
top_tri_grams_table = tabulate(top_tri_grams, headers=['Tri-gram', 'Frequency'], tablefmt='grid')
top_quad_grams_table = tabulate(top_quad_grams, headers=['Tri-gram', 'Frequency'], tablefmt='grid')

# Print the tables
print("Top 10 frequent words for ratings 1 and 2:")
print(top_words_table)
print("\nTop 10 frequent bi-grams for ratings 1 and 2:")
print(top_bi_grams_table)
print("\nTop 10 frequent tri-grams for ratings 1 and 2:")
print(top_tri_grams_table)
print("\nTop 10 frequent quad-grams for ratings 1 and 2:")
print(top_quad_grams_table)

"""### Insights

Here we are focusing on the terms that indicate most frequent complains, so the bank can understand quickly and improve the areas of pain to the large segment of customer at first.

*   In general we can observe the frequency of word combination for the negative reviews indicates most of the complaints are regarding customer service through calls
*   The other most frequent word is for 'credit card' and 'debit card'
*  Crtical phrases noticed are for' consumer financial protection bureau'

We can further apply other techniques to understand more about the customer behaviour and their queries regarding product and services of each bank

## Topic Modeling using LDA
Latent Dirichlet allocation (LDA) is a Bayesian network (and, therefore, a generative statistical model) for modeling automatically extracted topics in textual corpora.
"""

import gensim

tokens = nltk.word_tokenize(reviews_rating_1_2_filtered_cleaned)

reviews_rating_1_2_filtered_cleaned

tokens

sent_token=sent_tokenize(reviews_rating_1_2_joined)

reviews_rating_1_2_joined

sent_token[:10]

#converted to sentences

sent_token

tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sent_token]

# Filter out stopwords
stop_words = set(stopwords.words('english'))
filtered_sentences = [[word for word in sentence if word.lower() not in stop_words] for sentence in tokenized_sentences]

# Join filtered tokens back into sentences
filtered_sentences_joined = [' '.join(sentence) for sentence in filtered_sentences]

filtered_sentences_joined

import re

# Define the regular expression pattern to match non-alphabetic characters and whitespaces
special_chars_pattern = re.compile(r'[^a-zA-Z\s]')

# List to store cleaned documents
cleaned_documents = []

# Loop through each document in the documents list
for document in filtered_sentences_joined:
    # Remove special characters from the document
    cleaned_document = special_chars_pattern.sub('', document)
    # Append cleaned document to the list
    cleaned_documents.append(cleaned_document)

cleaned_documents[:3]

from gensim import corpora
from gensim.models import LdaModel
from gensim.models import CoherenceModel

tokenized_documents = [doc.split() for doc in cleaned_documents]

# Create a dictionary
id2word = corpora.Dictionary(tokenized_documents)



# Create Dictionary
id2word = corpora.Dictionary(tokenized_documents)

# Create Corpus
#texts = tokens

corpus = [id2word.doc2bow(doc) for doc in tokenized_documents]

corpus[:4]

lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=8,
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto',
                                           per_word_topics=True
                                        )

doc_lda = lda_model[corpus]

"""**Compute Perplexity and Coherence Score**"""

print('\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.

coherence_model_lda = CoherenceModel(model=lda_model, texts=tokenized_documents, dictionary=id2word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)

"""### Visualize the Topics"""

pip install pyLDAvis

import pyLDAvis.gensim

# Commented out IPython magic to ensure Python compatibility.
# %%time
# pyLDAvis.enable_notebook()
# vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)
# vis
# 
# #can convert these to bigrams for more interpretability
# #need to fine tune to get better interpreation of topics

"""## Word Clouds
Wordcloud of Top N words in each topic

"""

from PIL import Image
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator

stop_words = stopwords.words('english')

from matplotlib import pyplot as plt
from wordcloud import WordCloud, STOPWORDS
import matplotlib.colors as mcolors

cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]

cloud = WordCloud(stopwords=stop_words,
                  background_color='white',
                  width=2500,
                  height=1800,
                  max_words=100,
                  colormap='tab10',
                  color_func=lambda *args, **kwargs: cols[i],
                  prefer_horizontal=1.0)

topics = lda_model.show_topics(formatted=False,
                               num_words=30)

fig, axes = plt.subplots(5, 1, figsize=(10,20), sharex=True, sharey=True)

for i, ax in enumerate(axes.flatten()):
    fig.add_subplot(ax)
    topic_words = dict(topics[i][1])
    cloud.generate_from_frequencies(topic_words, max_font_size=500)
    plt.gca().imshow(cloud)
    plt.gca().set_title('Topic ' + str(i + 1), fontdict=dict(size=16))
    plt.gca().axis('off')


plt.subplots_adjust(wspace=0, hspace=0)
plt.axis('off')
plt.margins(x=0, y=0)
plt.tight_layout()
plt.show()

"""## Analysing Influential Author's Review
By observing the dataset we categorize the influential authors as thos having more than 5o likes on their reviews
"""

! pip install rake_nltk

import spacy
from rake_nltk import Rake

dataset_df.head()

likes_count = sum(1 for like_value in dataset_df['like'] if like_value > 40)

# Print the frequency
print(f"Frequency of likes more than 40: {likes_count}")

"""We have 153 authors categorized as influential.We will apply keyword extraction techniques to understand the sentiment of the text.

### TF-IDF Method
"""

import re

# Text Preprocessing

filtered_df = dataset_df[(dataset_df['like'] >= 40) & ((dataset_df['star'] == 1) | (dataset_df['star'] == 2))].dropna()
influence_text=filtered_df['text']

influence_text

def preprocess_text(text):
    # Convert text to lowercase
    text = text.lower()
    # Remove special characters
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Tokenize the text
    tokens = nltk.word_tokenize(text)
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word for word in tokens if word not in stop_words]
    # Join the tokens back into a string
    preprocessed_text = ' '.join(filtered_tokens)
    return preprocessed_text

# Apply preprocessing to each text in 'filtered_df'
cleaned_text = influence_text.apply(preprocess_text)

# Print the preprocessed text
print(cleaned_text)

print(type(cleaned_text))

#converted to list
updated_text = [cleaned_text.tolist()]

updated_text

cleaned_text_str = cleaned_text.str.cat(sep=' ')

"""### NER Model"""

nlp = spacy.load("en_core_web_sm")

# Process the text with spaCy
doc = nlp(cleaned_text_str)

# Extract named entities from the text
entities = [(ent.text, ent.label_) for ent in doc.ents]

# Print the extracted keywords and named entities

print("Named Entities:", entities)

product_org_entities = [(ent.text, ent.label_) for ent in doc.ents if ent.label_ in {"PRODUCT", "ORG"}]

# Print the extracted product and organization entities
print("Product and Organization Entities:")
for entity, label in product_org_entities:
    print(f"{entity}: {label}")

from sklearn.feature_extraction.text import TfidfVectorizer

flattened_text = [text for sublist in updated_text for text in sublist]

tfidf_vectorizer = TfidfVectorizer(max_df=0.6, min_df=2, stop_words='english')
tfidf_matrix = tfidf_vectorizer.fit_transform(flattened_text)
feature_names = tfidf_vectorizer.get_feature_names_out()
tfidf_scores = tfidf_matrix.toarray()

# Extract keywords based on TF-IDF scores
tfidf_keywords = []
for i, review in enumerate(updated_text):
    review_keywords = [feature_names[idx] for idx in tfidf_scores[i].argsort()[-20:][::-1]]  # Extract top 5 keywords
    tfidf_keywords.append(review_keywords)

"""* Below are the keywords less frequent but can give some imporatant or critical information,for example we the usage of word 'Scam', 'Rates', 'application', indicating areas of improvement which are of immediate importance








"""

review_keywords

def frequent_words(reviews, n=20, ngram=1):
    tokens = nltk.word_tokenize(reviews.lower())
    if ngram == 1:
        grams = tokens
    elif ngram == 2:
        grams = list(ngrams(tokens, 2))
    elif ngram == 3:
        grams = list(ngrams(tokens, 3))
    elif ngram == 4:
        grams = list(ngrams(tokens, 4))

    freq_dist = Counter(grams)
    top_n = freq_dist.most_common(n)
    return top_n

# Filter dataset for ratings 1 and 2
#reviews_rating_1_2 = ' '.join(review['text'] for review in dataset_df if review['star'] in [1, 2])

# Get the most frequent words, bi-grams, and tri-grams for ratings 1 and 2
top_words = frequent_words(cleaned_text_str, n=20, ngram=1)
top_bi_grams = frequent_words(cleaned_text_str, n=20, ngram=2)
top_tri_grams = frequent_words(cleaned_text_str, n=20, ngram=3)
top_quad_grams= frequent_words(cleaned_text_str, n=20, ngram=4)

# Convert the results into a presentable table format
top_words_table = tabulate(top_words, headers=['Word', 'Frequency'], tablefmt='grid')
top_bi_grams_table = tabulate(top_bi_grams, headers=['Bi-gram', 'Frequency'], tablefmt='grid')
top_tri_grams_table = tabulate(top_tri_grams, headers=['Tri-gram', 'Frequency'], tablefmt='grid')
top_quad_grams_table = tabulate(top_quad_grams, headers=['Tri-gram', 'Frequency'], tablefmt='grid')

# Print the tables
print("Top 10 frequent words for ratings 1 and 2:")
print(top_words_table)
print("\nTop 10 frequent bi-grams for ratings 1 and 2:")
print(top_bi_grams_table)
print("\nTop 10 frequent tri-grams for ratings 1 and 2:")
print(top_tri_grams_table)
print("\nTop 10 frequent quad-grams for ratings 1 and 2:")
print(top_quad_grams_table)

"""* Here we can observe the frequent words used by influential author
* This indicate some major customer grievances which other latent customer also agrees with.
* Important keyword observed are: 'Hostage', 'hacked', 'Rebuild', 'hard earned money', 'Credit card'

### Using Bertopic for Topic Modeling
"""

!pip install bertopic

from bertopic import BERTopic

model = BERTopic(verbose=True,embedding_model='paraphrase-MiniLM-L3-v2', min_topic_size= 9)
headline_topics, _ = model.fit_transform(cleaned_text)

freq = model.get_topic_info()
print("Number of topics: {}".format( len(freq)))
freq.head()

model.visualize_barchart(top_n_topics=10)

"""### Search Topics"""

similar_topics, similarity = model.find_topics("transfer", top_n = 3)

most_similar = similar_topics[0]
print("Most Similar Topic Info: \n{}".format(model.get_topic(most_similar)))
print("Similarity Score: {}".format(similarity[0]))